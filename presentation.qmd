---
title: "XGBoost: A Scalable Tree Boosting System"
subtitle: "@XGBoost"
author: "Christian Bager Bach Houmann"
date: 2023-09-22
date-format: "D MMM YYYY"
slide-number: true
bibliography: static/references.bib
format: 
    revealjs:
        theme: default
        toc: true
---

# By the end of this talk... {.unlisted}

You should understand every word of

> XGBoost: A Scalable Tree Boosting

And why you should use XGBoost

# Background

*From Zero to Gradient Boosting*

## Supervised Learning {.smaller}

- Supervised learning is where your training data has the desired solutions (labels).
- The model learns a function that maps inputs to correct outputs, which it can use on new, unlabled data to make accurate predictions.

<br />

::: {.hidden}
| Attendance Rate (%) | Hours Studied Per Week | Past Grades (GPA) | Participated in Group Study | [Passed course]{style="color: red"} |
|---------------|---------------|---------------|---------------|---------------|
| 90                  | 10                     | 3.5               | Yes                         | Yes                                 |
| 75                  | 4                      | 2.0               | No                          | No                                  |
| 85                  | 8                      | 3.0               | No                          | Yes                                 |
| 60                  | 3                      | 1.5               | No                          | No                                  |
| 95                  | 12                     | 3.8               | Yes                         | Yes                                 |
| 80                  | 6                      | 2.5               | Yes                         | No                                  |

: Simple supervised learning problem data
:::

![Taken from 5th semester MI course at AAU](static/images/tabular_data.png){fig-align="center"}

## Defining an objective {.unlisted}
- How do we measure how well the model fit the data?
$$
obj(\theta)=L(\theta)+\Omega(\theta)
$$
- Objective function = training loss + regularization term
- **Training loss:** how predictive our model is
- **Regularization:** helps prevent overfitting by penalizing complexity
- **The goal:** a simple yet predictive model

::: aside
$\theta$ are model parameters, $L$ is the loss function, and $\Omega$ is the regularization term.
:::


## Decision Trees {.smaller}

::: {layout-ncol="2"}
![](static/images/tabular_data.png)

![](static/images/decision_trees.png)
:::

::: aside
Taken from 5th semester MI course.
:::

::: {.hidden}
| Preferred Activity | Interest in History | Label       |
|--------------------|---------------------|-------------|
| Indoor             | Yes                 | Non-Fiction |
| Outdoor            | No                  | Fiction     |
| Indoor             | No                  | Fiction     |
| Outdoor            | Yes                 | Non-Fiction |
| Indoor             | Yes                 | Non-Fiction |

<br />

```{mermaid}
graph TD
    A[Preferred Activity?] -->|Indoor| B[Interest in History?]
    A -->|Outdoor| C[Interest in History?]
    
    B -->|Yes| D[Read Non-Fiction]
    B -->|No| E[Read Fiction]
    
    C -->|Yes| F[Read Non-Fiction]
    C -->|No| G[Read Fiction]
```
:::

## Ensemble Learning

-   Simple models like Decision Trees and Linear Regression are limited
-   We can boost performance with ensemble learning
-   Instead of creating complex model:
    -   Create lots of simple models (w. **weak learners**)
    -   Combine them into **meta-model**
-   The output is a weighted voting of the output from each simple model

::: notes
The simple models I refer to here are called **weak learners**. They aren't poor performing, but they are just made to be underpowered.
:::

## Boosting

We can boost by sequentially training and combining weak learners, each trying to correct its predecessor.

1.  Start with a weak learner
2.  Calculate the errors of this initial model
3.  Fit another weak learner, this time focusing on the errors made by the previous model
4.  Combine the weak models through weighted averaging
5.  Repeat 2-4 until we have enough models or no further improvements can be made

## Boosting {.unlisted}

Boosting is where you learn $F(x)$ as the sum of $M$ weak learners.
$$
F(x)=\sum^{M}_{i=1}f_{i}(x)
$$ 

where $f_{i}(x)$ is the weak learner.

## Gradient Boosted Trees

Original paper on Gradient Boosting is by @gb.

# eXtreme Gradient Boosting

## Let's try it! {.unlisted .smaller}

-   Using the [IRIS dataset from Scikit-Learn](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
-   We get insanely good classifications out of the box
-   Can't get easier than this

::: {layout-ncol="1" layout-halign="center"}
```{python}
#| fig-align: center
#| fig-cap: Using 80/20 holdout
from xgboost import XGBClassifier
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2, random_state=42)
bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')

bst.fit(X_train, y_train)

preds = bst.predict(X_test)

cm = confusion_matrix(y_test, preds)
sns.heatmap(cm, annot=True, fmt="d")
plt.title("Confusion Matrix")
plt.show()
```
:::

## What is XGBoost?

-   XGBoost stands for eXtreme Gradient Boosting
-   Machine Learning algorithm
-   Based on Gradient Boosting

## Why XGBoost?

# Conclusion

## Two Strong Points of the Article

## Two Weak Points of the Article

## Take-Home Message