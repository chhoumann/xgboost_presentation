@inproceedings{XGBoost,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  date = {2016-08-13},
  series = {{{KDD}} '16},
  pages = {785--794},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2939672.2939785},
  url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
  urldate = {2023-09-13},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  isbn = {978-1-4503-4232-2},
  keywords = {üìñ,üìö,large-scale machine learning},
  file = {C\:\\Users\\chhou\\Zotero\\storage\\DH52BMF9\\Chen_Guestrin_2016_XGBoost.pdf;C\:\\Users\\chhou\\Zotero\\storage\\LERKEA7G\\Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf}
}

@article{gb,
  title = {Greedy Function Approximation: {{A}} Gradient Boosting Machine.},
  shorttitle = {Greedy Function Approximation},
  author = {Friedman, Jerome H.},
  date = {2001-10},
  journaltitle = {The Annals of Statistics},
  volume = {29},
  number = {5},
  pages = {1189--1232},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1013203451},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full},
  urldate = {2023-09-21},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent ‚Äúboosting‚Äù paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such ‚ÄúTreeBoost‚Äù models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  keywords = {üìö,62-02,62-07,62-08,62G08,62H30,68T10,boosting,decision trees,Function estimation,robust nonparametric regression},
  file = {C\:\\Users\\chhou\\Zotero\\storage\\IQLVJYMB\\Friedman_2001_Greedy function approximation.pdf;C\:\\Users\\chhou\\Zotero\\storage\\ITW7X9P3\\Friedman - 2001 - Greedy function approximation A gradient boosting.pdf}
}
