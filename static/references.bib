@inproceedings{XGBoost,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  date = {2016-08-13},
  series = {{{KDD}} '16},
  pages = {785--794},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2939672.2939785},
  url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
  urldate = {2023-09-13},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  isbn = {978-1-4503-4232-2},
  keywords = {ðŸ“–,ðŸ“š,large-scale machine learning},
  file = {C\:\\Users\\chhou\\Zotero\\storage\\DH52BMF9\\Chen_Guestrin_2016_XGBoost.pdf;C\:\\Users\\chhou\\Zotero\\storage\\LERKEA7G\\Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf}
}
