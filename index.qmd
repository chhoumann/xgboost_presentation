---
title: "XGBoost: A Scalable Tree Boosting System"
subtitle: "@XGBoost"
author: "Christian Bager Bach Houmann"
date: 2023-09-22
date-format: "D MMM YYYY"
slide-number: true
bibliography: static/references.bib
format: 
    revealjs:
        theme: default
        toc: false
---

# By the end of this talk... {.unlisted}

You should understand every word of

> XGBoost: A Scalable Tree Boosting

And why you should use XGBoost!

# Let's try it! {.unlisted .smaller}

-   Using the [IRIS dataset from Scikit-Learn](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html): small dataset with iris (flower) data
-   We get very good classifications out of the box

:::: {layout-ncol="2" layout-valign="center" .columns}
:::{.column width="40%"}
```{.python code-line-numbers="1,6-13"}
from xgboost import XGBClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    data['data'], data['target'], 
    test_size=.2, random_state=42
)
bst = XGBClassifier()

bst.fit(X_train, y_train)
preds = bst.predict(X_test)
```
:::

:::{.column width="60%"}

```{python}
#| fig-align: center
from xgboost import XGBClassifier
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2, random_state=42)
bst = XGBClassifier()

bst.fit(X_train, y_train)

preds = bst.predict(X_test)

cm = confusion_matrix(y_test, preds)
sns.heatmap(cm, annot=True, fmt="d")
plt.title("Confusion Matrix")
plt.show()
```
:::
::::

# Background

*From Zero to Gradient Boosting*

## Supervised Learning {.smaller}

- Supervised learning is where your training data has the desired solutions (labels).
- The model learns a function that maps inputs to correct outputs, which it can use on new, unlabled data to make accurate predictions.

<br />

![Taken from 5th semester MI course at AAU](static/images/tabular_data.png){fig-align="center"}

## Defining an objective {.unlisted}
- How do we measure how well the model fit the data?
$$
obj(\theta)=L(\theta)+\Omega(\theta)
$$
- Objective function = training loss + regularization term
- **Training loss:** how predictive our model is
- **Regularization:** helps prevent overfitting by penalizing complexity
- **The goal:** a simple yet predictive model

::: aside
$\theta$ are model parameters, $L$ is the loss function, and $\Omega$ is the regularization term.
:::

## Decision Trees {.smaller}

::: {layout-ncol="2"}
![](static/images/tabular_data.png)

![](static/images/decision_trees.png)
:::

::: aside
Taken from 5th semester MI course at AAU.
:::

::: {.notes}
XGBoost uses CART: Classification and Regression Trees. I show decision trees to illustrate.

These are different from decision trees in that there is a real score associated with each leaf. The leaf only contains this decision value.
:::


## Ensemble Learning

-   Simple models like Decision Trees and Linear Regression are limited
-   We can boost performance with ensemble learning
-   Instead of creating complex model:
    -   Create lots of simple models (w. **weak learners**)
    -   Combine them into a **meta-model**
-   The output is a weighted voting of the output from each simple model

::: notes
**Weak learners** are learning algorithms that cannot learn complex models, so they are typically fast at training and prediction time.

 The most frequently used weak learner is a Decision Tree learning algorithm that we often stop splitting the training set after few iterations.
 
 The trees we get out are shallow and not very accurate, but still better than random guessing, so the combined model is a lot better.
:::

## Boosting

We can boost by sequentially training and combining weak learners, each trying to correct its predecessor.

1.  Start with a weak learner
2.  Calculate the errors of this initial model
3.  Fit another weak learner, this time focusing on the errors made by the previous model
4.  Combine the weak models through weighted averaging
5.  Repeat 2-4 until we have enough models or no further improvements can be made

## Boosting {.unlisted}

Boosting is where you learn $F(x)$ as the sum of $M$ weak learners.
$$
F(x)=\sum^{M}_{i=1}f_{i}(x)
$$ 

where $f_{i}(x)$ is the weak learner.

## Gradient Boosted Trees

Original paper on Gradient Boosting is by @gb.

# eXtreme Gradient Boosting

## What is XGBoost?

-   XGBoost stands for eXtreme Gradient Boosting
-   Machine Learning algorithm
-   Based on Gradient Boosting

## Why XGBoost?

# Conclusion

## Two Strong Points of the Article

## Two Weak Points of the Article

## Take-Home Message