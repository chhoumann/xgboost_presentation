---
title: "XGBoost: A Scalable Tree Boosting System"
subtitle: "@XGBoost"
author: "Christian Bager Bach Houmann"
date: 2023-09-22
date-format: "D MMM YYYY"
slide-number: true
bibliography: static/references.bib
format: 
    revealjs:
        theme: default
        toc: false
---

# By the end of this talk... {.unlisted}

You should understand every word of

> XGBoost: A Scalable Tree Boosting

And why you should use XGBoost!

# Let's try it! {.unlisted .smaller}

-   Using the [IRIS dataset from Scikit-Learn](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html): small dataset with iris (flower) data
-   We get very good classifications out of the box

:::: {layout-ncol="2" layout-valign="center" .columns}
:::{.column width="40%"}
```{.python code-line-numbers="1,6-13"}
from xgboost import XGBClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    data['data'], data['target'], 
    test_size=.2, random_state=42
)
bst = XGBClassifier()

bst.fit(X_train, y_train)
preds = bst.predict(X_test)
```
:::

:::{.column width="60%"}

```{python}
#| fig-align: center
from xgboost import XGBClassifier
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2, random_state=42)
bst = XGBClassifier()

bst.fit(X_train, y_train)

preds = bst.predict(X_test)

cm = confusion_matrix(y_test, preds)
sns.heatmap(cm, annot=True, fmt="d")
plt.title("Confusion Matrix")
plt.show()
```
:::
::::

# Background

*From Zero to Gradient Boosting*

## Supervised Learning {.smaller}

- Supervised learning is where your training data has the desired solutions (labels).
- The model learns a function that maps inputs to correct outputs, which it can use on new, unlabled data to make accurate predictions.

<br />

![Taken from 5th semester MI course at AAU](static/images/tabular_data.png){fig-align="center"}

## Defining an objective {.unlisted}
- How do we measure how well the model fit the data?
$$
obj(\theta)=L(\theta)+\Omega(\theta)
$$
- Objective function = training loss + regularization term
- **Training loss:** how predictive our model is
- **Regularization:** helps prevent overfitting by penalizing complexity
- **The goal:** a simple yet predictive model

::: aside
$\theta$ are model parameters, $L$ is the loss function, and $\Omega$ is the regularization term.
:::

## Decision Trees {.smaller}

::: {layout-ncol="2"}
![](static/images/tabular_data.png)

![](static/images/decision_trees.png)
:::

::: aside
Taken from 5th semester MI course at AAU.
:::

::: {.notes}
XGBoost uses CART: Classification and Regression Trees. I show decision trees to illustrate.

These are different from decision trees in that there is a real score associated with each leaf. The leaf only contains this decision value.
**So the leaf scores are continuous!**
:::


## Ensemble Learning

-   Simple models like Decision Trees and Linear Regression are limited
-   We can boost performance with ensemble learning
-   Instead of creating complex model:
    -   Create lots of simple models (w. **weak learners**)
    -   Combine them into a **meta-model**
-   The output is a weighted voting of the output from each simple model

::: notes
**Weak learners** are learning algorithms that cannot learn complex models, so they are typically fast at training and prediction time.

 The most frequently used weak learner is a Decision Tree learning algorithm that we often stop splitting the training set after few iterations.
 
 The trees we get out are shallow and not very accurate, but still better than random guessing, so the combined model is a lot better.
:::

## Boosting

We can boost by sequentially training and combining weak learners, each trying to correct its predecessor.

1.  Start with a weak learner
2.  Calculate the errors of this initial model
3.  Fit another weak learner, this time focusing on the errors made by the previous model
4.  Combine the weak models through weighted averaging
5.  Repeat 2-4 until we have enough models or no further improvements can be made

## Boosting {.unlisted}

Boosting is where you learn $F(x)$ as the sum of $M$ weak learners.
$$
F(x)=\sum^{M}_{i=1}f_{i}(x)
$$ 

where $f_{i}(x)$ is the weak learner.

## Gradient Boosted Trees
- Same as boosting, except:
- Instead of fitting to errors from prev. trees, we fit to the negative gradients w.r.t the prediction of the prev. trees.

::: aside
The original paper on Gradient Boosting is by @gb.
:::

::: {.notes}
These negative gradients represent the direction in which the model should change its predictions to minimize the loss. 

The use of gradient descent as an optimization framework also sets it apart from other boosting algorithms.
:::


# eXtreme Gradient Boosting
*Putting it all together...*

## Contributions
- Scalable, end-to-end tree boosting system^[End-to-end means it takes raw input and produces final output - no relying on intermediary or hand-coded steps]
- Theoretically justified weighted quantile sketch for efficient proposal calculation^[It's efficient at finding optimal splits]
- Sparsity-aware algorithm for parallel tree learning^[Sparsity means not all values are stored - e.g. $0$s or `NULL`s are not in $X$ training data]
- Cache-aware block structure for out-of-core tree learning^[Out-of-core means that we learn (via online learning) on mini-batches of training data.]

::: {.notes}
- **Scalable**: it works well with large datasets
- **End-to-end**: this is true, but not in the same way deep learning algorithms are end-to-end - they can take raw images/raw text, xgb cannot
- **Weighted quantile sketch**: this is what makes xgboost faster and more accurate than some other gradient boosting algorithms
- **Sparsity-aware**: this is also useful for high dimensionality data with missing values
- **Parallel**: both parallel computation during tree construction and boosting rounds
- **Cache aware**: is because XGBoost has hardware-optimized data structures that are cache efficient.
- **Out-of-core**: XGBoost doesn't use online learning - for XGBoost, this happens by storing data in binary files and loading chunks during learning.
:::

## How XGBoost learns trees
Our model is of the form

$$
\hat{y}_{i}=\sum^{K}_{k=1}f_{k}(x_{i}),\quad f_{k}\in\mathcal{F}
$$

where

- $K$ is the number of trees,
- $f_k$ is a function in the functional space $\mathcal{F}=\{ f(x)=w_{q(x)} \}$, which is the set of all possible CARTs,
- $q$ maps a sample to a leaf,
- and $w_j$ is the weight for a leaf $j$.

## How XGBoost learns trees {.unlisted}
The idea is we try to optimize

$$
obj(\theta)=\sum^{n}_{i}l(y_{i},\hat{y}_{i})+\sum^{K}_{k=1}\omega(f_{k})
$$
where $\omega(f_{k})$ is the complexity of the tree $f_{k}$.

But... how do we actually learn the trees? What about their structure and leaf scores?

::: aside
As you may recall: objective function = loss function + regularization term.

I'll talk about the complexity later.
:::

::: {.notes}
Unfortunately, it's not as easy as just optimizing this function.

We need to learn the functions $f_i$ which each contains the structure of the tree and leaf scores.
:::

## How XGBoost learns trees {.unlisted}
We learn the trees additively!

$$
\begin{align}
&\hat{y}^{(0)}_{i}=0 \\
&\hat{y}^{(1)}_{i}=f_{1}(x_{i})=\hat{y}_{i}^{(0)}+f_{1}(x_{i}) \\
&\hat{y}^{(2)}_{i}=f_{1}(x_{i})+f_{2}(x_{i})=\hat{y}_{i}^{(1)}+f_{2}(x_{i}) \\
&\quad\cdots \\
& \hat{y}^{(t)}_{i}=\sum^{t}_{k=1}f_{k}(x_{i})=\hat{y}_{i}^{(t-1)}+f_{t}(x_{i})
\end{align}
$$

::: {.notes}
So the prediction is going to be the sum of all tree functions, which is the same as all the previous predictions + our latest tree's prediction.
:::

## How do we know which tree we want at each step? {.unlisted}
The one that optimizes our objective:

$$
\begin{align}
obj(t) &= \sum^{n}_{i=1}l(y_{i},\hat{y}^{(t)}_{i})+\sum^{t}_{i=1}\omega(f_{i}) \\
&=\sum^{n}_{i=1}l(y_{i},\hat{y}^{(t-1)}_{i}+f_{t}(x_{i}))+\omega(f_{t})+\text{constant}
\end{align}
$$

However, we can't optimize this objective function with traditional methods in Euclidean space - because our model takes functions as parameters.

::: {.notes}
So we approximate it with Taylor expansion to the second-order to optimize for the general loss functions.

This makes it possible to optimize as there are some loss functions you otherwise couldn't optimize â€“ we make it general, so you can use arbitrary loss functions, as long as they are differentiable.
:::

## And what about the regularization term? {.unlisted}
Remember our definition of a tree:
$$
f_{t}(x)=w_{q(x)},\quad w\in \mathbb{R}^{T}, q:\mathbb{R}^{d}\rightarrow \{1,2,\dots ,T\}
$$
$w$ is the vector score on leaves, $q$ is a function that assign each data point to the corresponding leaf, and $T$ is the number of leaves.

We define the complexity as
$$
\omega(f)=\gamma T+ \frac{1}{2}\lambda \sum^{T}_{j=1}w^{2}_{j}
$$


::: {.notes}
Now that we know the training step, we need to define the regularization

$\gamma$ is the minimum gain to make a split, which helps us prevent the model from making too many splits unless they improve the fit.

$\lambda$ is to regularize the leaf weights. They capture some pattern from the training data, but if they're too large/too small, the model might fit the data too closely - we don't want it to remember all the noise. If $\lambda$ is too large we might underfit because the weights are too small.

So our complexity measure just regulates our model based on how many leaves it makes and ensures the weights don't get out of control.
:::

## Learning the tree structure {.unlisted}
::: aside
I'm skipping how we derive the structure function from the objective function
:::

Use the Exact Greedy Algorithm:

1. Start with single root - contains all training examples
2. Iterate over all features and values per feature and evaluate each possible split gain (see formula below)
3. The **gain** for the best split must be positive, otherwise we stop growing the branch

::: {.notes}
**START HERE**

- Cannot enumerate all possible tree structures...
- So we find the best tree by optimizing one level at a time

The algorithm here is the Exact Greedy Algoritm we use.

**AFTER SLIDE**
Let's review the gain.
:::

## Gain {.unlisted}
We try to split a leaf into two leaves, and the score it gains is
$$
Gain=\frac{1}{2}\left[ \frac{G^{2}_{L}}{H_{L}+\lambda}+ \frac{G^{2}_{R}}{H_{R}+\lambda}-\frac{(G_{L}+G_{R})^{2}}{H_{L}+H_{R}+\lambda} \right]-\gamma
$$

1. The score on the new left leaf
2. The score on the new right leaf
3. The score if we do not split
4. The complexity cost by adding additional leaf

::: {.notes}
First: gain is loss reduction - so it's a measure of whether we actually improve the model by splitting.

1. The score on the new left leaf $\frac{G^{2}_{L}}{H_{L}+\lambda}$
2. The score on the new right leaf $\frac{G^{2}_{R}}{H_{R}+\lambda}$
3. The score if we do not split $\frac{(G_{L}+G_{R})^{2}}{H_{L}+H_{R}+\lambda}$
4. The complexity cost by adding additional leaf $\gamma$

If the gain is smaller than $\gamma$, we shouldn't add that branch (it'll be negative). This is the pruning technique used in tree-based models.
:::

## Further prevention of overfitting
XGBoost also uses


## Why XGBoost

# Conclusion

## Two Strong Points of the Article

## Two Weak Points of the Article

## Take-Home Message